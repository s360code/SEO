# -*- coding: utf-8 -*-
"""Keyword Difficulty.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1epYpSEQh06DZdGDoH-Rbysi5T2mTQUj5
"""
from gspread_dataframe import get_as_dataframe, set_with_dataframe
from google.auth.transport.requests import AuthorizedSession
#from oauth2client.client import GoogleCredentials
import pandas as pd
import google.auth
import gspread
from urllib.request import urlopen, Request
import json
import pandas as pd
from datetime import datetime
from dateutil.relativedelta import relativedelta
import calendar
import requests
from datetime import date
import concurrent.futures
import itertools
import re
import numpy as np
import math
from requests.auth import HTTPBasicAuth

"""Competition metric - Keyword Difficulty

Using Bulk Keyword Difficulty endpoint
"""

class KeywordDiffiulty:
    def __init__(self, sheet_id, gc):
        self.sheet_id = sheet_id
        self.gc = gc
        self.connection = self.gc.open_by_url("https://docs.google.com/spreadsheets/d/"+str(self.sheet_id))
        self.sheet = self.connection.worksheet('Keyword Analysis')
        self.start = 4

    def data_from_sheet(self):
        dataframe = get_as_dataframe(self.sheet)
        dataframe.columns = dataframe.iloc[self.start, :].tolist()
        dataframe = dataframe.drop(range(self.start+1))
        return dataframe
        
    def get_relevant_keywords(self):
        keyword_analysis_df = self.data_from_sheet()
        if "Keyword difficulty" in keyword_analysis_df.columns:
            keyword_analysis_df = keyword_analysis_df[keyword_analysis_df["Keyword difficulty"].isna()]
            return keyword_analysis_df.iloc[:, 0]
        else:
            return keyword_analysis_df.iloc[:, 0]

    def filter_keywords(self):
        keyword_list = self.get_relevant_keywords()
        keyword_list = [str(x) for x in keyword_list]
        keyword_list = [x for x in keyword_list if x]
        blacklist = ["?", "@", "#", "'"]
        keyword_list = [word for word in keyword_list if not any(bad in word for bad in blacklist)]
        return keyword_list

    def divide_chunks(self):
        keyword_list = self.filter_keywords()
        keyword_list_chunks = []
        for i in range(0, len(keyword_list), 999):
            keyword_list_chunks.append(keyword_list[i:i + 999])
        return keyword_list_chunks

    def api_call(self):
      keyword_list_chunks = self.divide_chunks()
      location = "Denmark"
      language = "Danish"
      payload = dict()
      keywords_test= []
      keyword_dif = []
      for x in keyword_list_chunks:
          payload = dict()
          url = "https://api.dataforseo.com/v3/dataforseo_labs/google/bulk_keyword_difficulty/live"
          payload[len(payload)] = dict(
              keywords=x,
              location_name=location,
              language_name=language      
              
          )
          response = requests.post(url=url, data=json.dumps(payload), auth=HTTPBasicAuth("mcc@s360digital.com", "bcaa7a5a7ec8231a"))
          res = response.json()
          item = res['tasks'][0]['result'][0]['items']
          # Loop over the domains and extract the relevant data from the responses
          for response in item:
            try:
              keywords_test.append(response['keyword'])
            except:
              keywords_test.append('')
            try:
              keyword_dif.append(response['keyword_difficulty'])
            except:
              keyword_dif.append('')

      keyword_difficulty_df = pd.DataFrame({'Keyword':keywords_test,
                   'Keyword difficulty':keyword_dif})
      keyword_difficulty_df.drop_duplicates(subset=['Keyword'])
      return keyword_difficulty_df

    def postdata(self):

        keyword_analysis_df = self.data_from_sheet() # gets data again
        keyword_difficulty_df = self.api_call() #returns keyword_difficulty_df

        if "Keyword difficulty" in keyword_analysis_df.columns:
          filtered_df = keyword_analysis_df[["Keyword", "Keyword difficulty"]]
          alternative_df = pd.concat([filtered_df, keyword_difficulty_df], axis=0, join='outer', ignore_index=True, sort=False)
          alternative_df = alternative_df.drop_duplicates(subset=['Keyword'], keep='last')
          alternative_df = alternative_df[alternative_df['Keyword']!='']
          keyword_analysis_df = keyword_analysis_df.drop(columns=['Keyword difficulty'])
          new_df = keyword_analysis_df.merge(alternative_df, on='Keyword', how='left')
          set_with_dataframe(self.sheet, new_df, row=6, include_index=False, include_column_header=True)

        else:
          # new df for keyword analysis
          new_df = keyword_analysis_df.merge(keyword_difficulty_df, on='Keyword', how='left')
          new_df = new_df.drop_duplicates(subset=['Keyword'])
          self.sheet.add_cols(1)
          set_with_dataframe(self.sheet, new_df, row=6, include_index=False, include_column_header=True)

        try: 
            gc.insert_permission(
            self.connection.id,
            "s360digital.com",
            perm_type='domain',
            role='writer'
            )
        except:
            gc.insert_permission(
            self.connection.id,
            None,
            perm_type='anyone',
            role='reader'
              )

def main():
    auth.authenticate_user()
    creds, _ = default()
    gc = gspread.authorize(creds)
    test = KeywordDiffiulty('1gkH1gS7Vj0x70PVKMJZhzVLuuMoa3dl2RqzVNmAp3u8', gc)
    return test.data_from_sheet() # change to return something when deploying to cloud  
main()



